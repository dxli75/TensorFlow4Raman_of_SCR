{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import gc\n",
    "import time \n",
    "import scipy.io \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import tensorflow as tf\n",
    "import tqdm as notebook_tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import ravel\n",
    "from sklearn.preprocessing import OneHotEncoder,KBinsDiscretizer,LabelEncoder,OrdinalEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Now keras tensorflow 1D CNN for MRL+STD strians classification is carried out...\\n\")\n",
    "## Set memory_limit and GPU\n",
    "using_gpu_index = 0 # 使用的 GPU 号码\n",
    "gpu_list = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_list) > 0:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpu_list[using_gpu_index], \n",
    "            [tf.config.LogicalDeviceConfiguration(memory_limit=5120)]\n",
    "        )\n",
    "        print(\"We Got GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"Got no GPUs\")\n",
    "\n",
    "\n",
    "# load dataset\n",
    "data2 = pd.read_csv('../Raman_MRL+STD_data/Raman_MRL+STD_data_400to1799_preprocessed_strains_1.csv',sep=',', header=0, names=None, index_col=None)\n",
    "\n",
    "X2 = data2.iloc[:,:-1]\n",
    "Y = data2.iloc[:, -1]\n",
    "\n",
    "# Encode the labels into unique integers\n",
    "encoder = LabelEncoder()\n",
    "Y2 = encoder.fit_transform(ravel(Y))\n",
    "label_code = set(zip(Y2,Y))\n",
    "label_code = {k:v for (k,v) in sorted(label_code, key=lambda label_code : label_code[0])} \n",
    "#print(\"\\nlabel_code:\\n\",label_code)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "Y3 = encoder.fit_transform(Y2.reshape(-1, 1)).toarray()\n",
    "print(Y3)\n",
    "\n",
    "# label_code2 = set(zip(Y3,Y))\n",
    "# label_code2 = {k:v for (k,v) in sorted(label_code2, key=lambda label_code2 : label_code2[0])} \n",
    "# print(\"\\nlabel_code:\\n\",label_code2)\n",
    "\n",
    "X = np.array(X2)\n",
    "Y = np.array(Y)\n",
    "Y_CNN = Y3\n",
    "n_class = 199\n",
    "\n",
    "print('Shape of Input Data =', X.shape)\n",
    "print('Shape of Label Y_CNN =', Y_CNN.shape)\n",
    "print('Shape of Label Y =', Y.shape)\n",
    "print('Number of Classification n_class =', n_class)\n",
    "\n",
    "gc.collect()\n",
    "print(\"\\nDataset is Loaded.\\n\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\nStart training...\")\n",
    "\n",
    "kSplits = 15\n",
    "\n",
    "kfold = KFold(n_splits=kSplits, random_state=32, shuffle=True)\n",
    "Input_1D = X.reshape([-1,467,1])\n",
    "X_1D_train, X_1D_test, y_1D_train, y_1D_test = train_test_split(Input_1D, Y_CNN, train_size=0.8,test_size=0.2, random_state=101)\n",
    "\n",
    "class CNN_1D():\n",
    "    def __init__(self):\n",
    "        self.model = self.CreateModel()\n",
    "\n",
    "    def CreateModel(self):\n",
    "        att='relu'# The best activation of relu.\n",
    "        model = models.Sequential([\n",
    "            layers.Conv1D(filters=16, kernel_size=3, strides=1, activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv1D(filters=32, kernel_size=3, strides=1, activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv1D(filters=128, kernel_size=3, strides=1, activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv1D(filters=256, kernel_size=3, strides=1, activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Flatten(),\n",
    "            layers.InputLayer(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(1024,activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(512,activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(256,activation=att),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dense(199),\n",
    "            layers.Softmax()\n",
    "            ])\n",
    "    \n",
    "        model.compile(optimizer='adam', # adam ,adamax,Nadam,RMSprop,SGD   X Adadelta,adagrad\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "                  \n",
    "        #model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "earlystop = EarlyStopping(monitor=\"val_accuracy\", \n",
    "                                   min_delta=0.0001,\n",
    "                                   patience=200,\n",
    "                                   restore_best_weights=True)\n",
    "\n",
    "# 模型训练\n",
    "i = 0\n",
    "loss_1D = []\n",
    "accuracy_1D = []\n",
    "best_accuracy = 0.0\n",
    "for train, validation in kfold.split(X_1D_train,y_1D_train):\n",
    "    i += 1\n",
    "    print(\"\\nLoops: \",i)\n",
    "    if i== 1:\n",
    "        Classification_1D = CNN_1D()\n",
    "    else:\n",
    "        Classification_1D = tf.keras.models.load_model('Raman-keras-tf-1D-CNN-MRL+STD_data1_Strains_model.h5')\n",
    "        \n",
    "    history = Classification_1D.model.fit(X_1D_train[train], y_1D_train[train], \n",
    "                                        verbose=2, \n",
    "                                        epochs=2000\n",
    "                                        batch_size=32,\n",
    "                                        validation_data=(X_1D_train[validation], y_1D_train[validation]),\n",
    "                                        callbacks=[earlystop], \n",
    "                                        validation_freq=1,\n",
    "                                       )\n",
    "    kf_loss, kf_accuracy = Classification_1D.model.evaluate(X_1D_test, y_1D_test,\n",
    "                                                            verbose=1,\n",
    "                                                            batch_size=32,) \n",
    "    loss_1D.append(kf_loss)\n",
    "    accuracy_1D.append(kf_accuracy)\n",
    "    Classification_1D.model.save(\"Raman-keras-tf-1D-CNN-MRL+STD_data1_Strains_model-loops\"+str(i)+\"-\"+str(kf_accuracy)[:6]+\".h5\")\n",
    "    if kf_accuracy > best_accuracy: \n",
    "        del Classification_1D\n",
    "        gc.collect()\n",
    "        Classification_1D.model.save('Raman-keras-tf-1D-CNN-MRL+STD_data1_Strains_model.h5')\n",
    "        \n",
    "        \n",
    "print(\"\\nThe training is normally terminated.\\n\")  \n",
    "print(\"=\"*100)\n",
    "print(\"\\nShow finial training results:\")\n",
    "\n",
    "CNN_1D_train_loss = np.average(loss_1D)\n",
    "print('CNN 1D train loss =', CNN_1D_train_loss)\n",
    "\n",
    "CNN_1D_train_accuracy = np.average(accuracy_1D)*100\n",
    "print('CNN 1D train accuracy =', CNN_1D_train_accuracy)\n",
    "\n",
    "CNN_1D_test_loss, CNN_1D_test_accuracy = Classification_1D.model.evaluate(X_1D_test, y_1D_test)\n",
    "CNN_1D_test_accuracy*=100\n",
    "print('CNN 1D test loss =', CNN_1D_test_loss)\n",
    "print('CNN 1D test accuracy =', CNN_1D_test_accuracy)\n",
    "\n",
    "# 保存模型\n",
    "# Classification_1D.model.save('Raman-keras-tf-1D-CNN-MRL+STD_data1_Strains_model.h5')\n",
    "# filepath = \"Raman-keras-tf-1D-CNN-MRL+STD_data1_Strains_model-last.h5\" #模型写入名字带dict的epoch，val_accuracy，还可以是val_loss\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    " \n",
    "# 模型加载\n",
    "# best_model=tf.keras.models.load_model('Raman-keras-tf-1D-CNN-MRL+STD_data1_Strains_model.h5')\n",
    "\n",
    "def ConfusionMatrix(model,X,Y,job=\"train\"):\n",
    "    '''\n",
    "    '''\n",
    "    import time\n",
    "    \n",
    "    localtime = time.strftime(\"%Y%m%d%H%M\", time.localtime())\n",
    "    Y_true = np.argmax(Y, axis=1)\n",
    "    Y_pred = np.argmax(model.predict(X), axis=1)\n",
    "    print(classification_report(Y_true,Y_pred))\n",
    "    \n",
    "    ConfusionMat1 = confusion_matrix(Y_true,Y_pred)\n",
    "    print(ConfusionMat1.shape)\n",
    "    print(ConfusionMat1)\n",
    "\n",
    "    plt.figure(figsize=(int(ConfusionMat1.shape[0]/2),int(ConfusionMat1.shape[1]/2)))\n",
    "    plt.title('Confusion Matrix - '+job) \n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    sns.heatmap(ConfusionMat1,annot=True,fmt='d',annot_kws={\"fontsize\":7},cmap=\"Greens\")\n",
    "    plt.savefig(\"ConfusionMatrix_\"+job+\"_\"+localtime+\"-1.png\", dpi=300)\n",
    "    #plt.show()\n",
    "    \n",
    "    df_cm = pd.DataFrame(ConfusionMat1)\n",
    "    df_cm.to_csv(\"ConfusionMatrix_\"+job+\"_\"+localtime+\"-1.csv\")\n",
    "    \n",
    "    ConfusionMat2 = ConfusionMat1.astype(np.float64)\n",
    "    ConfusionMat2 /= np.sum(ConfusionMat2, axis=1)\n",
    "    ConfusionMat2 *= 100\n",
    "    print(ConfusionMat2)\n",
    "\n",
    "    plt.figure(figsize=(int(ConfusionMat2.shape[0]/2),int(ConfusionMat2.shape[1]/2)))\n",
    "    plt.title('Confusion Matrix - '+job) \n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    sns.heatmap(ConfusionMat2,annot=True,fmt=\"5.2f\",annot_kws={\"fontsize\":7},cmap=\"Greens\")   # for int data,fmt=\".d\",\n",
    "    plt.savefig(\"ConfusionMatrix_\"+job+\"_\"+localtime+\"-2.png\", dpi=300)\n",
    "    #plt.show()\n",
    "\n",
    "    df_cm = pd.DataFrame(ConfusionMat2)\n",
    "    df_cm.to_csv(\"ConfusionMatrix_\"+job+\"_\"+localtime+\"-2.csv\")\n",
    "\n",
    "    #return ConfusionMat1,ConfusionMat2\n",
    "        \n",
    "localtime = time.strftime(\"%Y%m%d%H%M\", time.localtime())\n",
    "\n",
    "ConfusionMatrix(Classification_1D.model, X_1D_test, y_1D_test, job=\"keras-tf-1D-CNN-test\" )\n",
    "ConfusionMatrix(Classification_1D.model, X_1D_train,y_1D_train,job=\"keras-tf-1D-CNN-train\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow210",
   "language": "python",
   "name": "tensorflow210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
